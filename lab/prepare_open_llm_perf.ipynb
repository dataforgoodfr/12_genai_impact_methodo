{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28bce233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b737d20",
   "metadata": {},
   "source": [
    "### Open LLM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408c1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm_df = pd.read_csv('./data/open_llm/open-llm-leaderboard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f42b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b77c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>Model</th>\n",
       "      <th>Average ‚¨ÜÔ∏è</th>\n",
       "      <th>ARC</th>\n",
       "      <th>HellaSwag</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>TruthfulQA</th>\n",
       "      <th>Winogrande</th>\n",
       "      <th>GSM8K</th>\n",
       "      <th>Type</th>\n",
       "      <th>Architecture</th>\n",
       "      <th>Weight type</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Merged</th>\n",
       "      <th>Hub License</th>\n",
       "      <th>#Params (B)</th>\n",
       "      <th>Hub ‚ù§Ô∏è</th>\n",
       "      <th>Available on the hub</th>\n",
       "      <th>Model sha</th>\n",
       "      <th>Flagged</th>\n",
       "      <th>MoE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üî∂</td>\n",
       "      <td>davidkim205/Rhea-72b-v0.5</td>\n",
       "      <td>81.22</td>\n",
       "      <td>79.78</td>\n",
       "      <td>91.15</td>\n",
       "      <td>77.95</td>\n",
       "      <td>74.50</td>\n",
       "      <td>87.85</td>\n",
       "      <td>76.12</td>\n",
       "      <td>fine-tuned on domain-specific datasets</td>\n",
       "      <td>LlamaForCausalLM</td>\n",
       "      <td>Original</td>\n",
       "      <td>float16</td>\n",
       "      <td>False</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>72.29</td>\n",
       "      <td>38.0</td>\n",
       "      <td>True</td>\n",
       "      <td>fda5cf998a0f2d89b53b5fa490793e3e50bb8239</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üí¨</td>\n",
       "      <td>Contamination/contaminated_proof_7b_v1.0_safet...</td>\n",
       "      <td>81.14</td>\n",
       "      <td>78.07</td>\n",
       "      <td>90.22</td>\n",
       "      <td>78.92</td>\n",
       "      <td>82.29</td>\n",
       "      <td>88.16</td>\n",
       "      <td>69.14</td>\n",
       "      <td>chat models (RLHF, DPO, IFT, ...)</td>\n",
       "      <td>MistralForCausalLM</td>\n",
       "      <td>Original</td>\n",
       "      <td>float16</td>\n",
       "      <td>False</td>\n",
       "      <td>unknown</td>\n",
       "      <td>7.24</td>\n",
       "      <td>9.0</td>\n",
       "      <td>True</td>\n",
       "      <td>5d7fcb3724d6b08cf82e1b0c1faa1695b9fd6932</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üí¨</td>\n",
       "      <td>Contamination/contaminated_proof_7b_v1.0</td>\n",
       "      <td>81.14</td>\n",
       "      <td>78.07</td>\n",
       "      <td>90.22</td>\n",
       "      <td>78.92</td>\n",
       "      <td>82.29</td>\n",
       "      <td>88.16</td>\n",
       "      <td>69.14</td>\n",
       "      <td>chat models (RLHF, DPO, IFT, ...)</td>\n",
       "      <td>MistralForCausalLM</td>\n",
       "      <td>Original</td>\n",
       "      <td>float16</td>\n",
       "      <td>False</td>\n",
       "      <td>unknown</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>b1415875faed65cd29fd804941f5dcf835e99608</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üî∂</td>\n",
       "      <td>davidkim205/Rhea-72b-v0.4</td>\n",
       "      <td>81.09</td>\n",
       "      <td>78.50</td>\n",
       "      <td>90.75</td>\n",
       "      <td>78.01</td>\n",
       "      <td>73.91</td>\n",
       "      <td>86.74</td>\n",
       "      <td>78.62</td>\n",
       "      <td>fine-tuned on domain-specific datasets</td>\n",
       "      <td>LlamaForCausalLM</td>\n",
       "      <td>Original</td>\n",
       "      <td>float16</td>\n",
       "      <td>False</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>72.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5502123c46485914a580d6794eeb5fb3554b46aa</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üí¨</td>\n",
       "      <td>MTSAIR/MultiVerse_70B</td>\n",
       "      <td>81.00</td>\n",
       "      <td>78.67</td>\n",
       "      <td>89.77</td>\n",
       "      <td>78.22</td>\n",
       "      <td>75.18</td>\n",
       "      <td>87.53</td>\n",
       "      <td>76.65</td>\n",
       "      <td>chat models (RLHF, DPO, IFT, ...)</td>\n",
       "      <td>LlamaForCausalLM</td>\n",
       "      <td>Original</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>False</td>\n",
       "      <td>other</td>\n",
       "      <td>72.29</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>ea2b4ff8e5acd7a48993f56b2d7b99e049eb6939</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   T                                              Model  Average ‚¨ÜÔ∏è    ARC  \\\n",
       "0  üî∂                          davidkim205/Rhea-72b-v0.5       81.22  79.78   \n",
       "1  üí¨  Contamination/contaminated_proof_7b_v1.0_safet...       81.14  78.07   \n",
       "2  üí¨           Contamination/contaminated_proof_7b_v1.0       81.14  78.07   \n",
       "3  üî∂                          davidkim205/Rhea-72b-v0.4       81.09  78.50   \n",
       "4  üí¨                              MTSAIR/MultiVerse_70B       81.00  78.67   \n",
       "\n",
       "   HellaSwag   MMLU  TruthfulQA  Winogrande  GSM8K  \\\n",
       "0      91.15  77.95       74.50       87.85  76.12   \n",
       "1      90.22  78.92       82.29       88.16  69.14   \n",
       "2      90.22  78.92       82.29       88.16  69.14   \n",
       "3      90.75  78.01       73.91       86.74  78.62   \n",
       "4      89.77  78.22       75.18       87.53  76.65   \n",
       "\n",
       "                                     Type        Architecture Weight type  \\\n",
       "0  fine-tuned on domain-specific datasets    LlamaForCausalLM    Original   \n",
       "1       chat models (RLHF, DPO, IFT, ...)  MistralForCausalLM    Original   \n",
       "2       chat models (RLHF, DPO, IFT, ...)  MistralForCausalLM    Original   \n",
       "3  fine-tuned on domain-specific datasets    LlamaForCausalLM    Original   \n",
       "4       chat models (RLHF, DPO, IFT, ...)    LlamaForCausalLM    Original   \n",
       "\n",
       "  Precision  Merged Hub License  #Params (B)  Hub ‚ù§Ô∏è Available on the hub  \\\n",
       "0   float16   False  apache-2.0        72.29    38.0                 True   \n",
       "1   float16   False     unknown         7.24     9.0                 True   \n",
       "2   float16   False     unknown         7.00     3.0                 True   \n",
       "3   float16   False  apache-2.0        72.29     0.0                False   \n",
       "4  bfloat16   False       other        72.29     6.0                 True   \n",
       "\n",
       "                                  Model sha  Flagged    MoE  \n",
       "0  fda5cf998a0f2d89b53b5fa490793e3e50bb8239    False  False  \n",
       "1  5d7fcb3724d6b08cf82e1b0c1faa1695b9fd6932     True  False  \n",
       "2  b1415875faed65cd29fd804941f5dcf835e99608     True  False  \n",
       "3  5502123c46485914a580d6794eeb5fb3554b46aa    False  False  \n",
       "4  ea2b4ff8e5acd7a48993f56b2d7b99e049eb6939    False  False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_llm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314b0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm_df.rename(columns={'Average ‚¨ÜÔ∏è': 'Average', 'Model': 'model'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f83152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm_df = open_llm_df[['model', 'Average', 'ARC', 'HellaSwag', 'MMLU', 'TruthfulQA', 'Winogrande', 'GSM8K', \n",
    "                  'Precision', '#Params (B)', 'Flagged', 'MoE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c216508",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Average</th>\n",
       "      <th>ARC</th>\n",
       "      <th>HellaSwag</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>TruthfulQA</th>\n",
       "      <th>Winogrande</th>\n",
       "      <th>GSM8K</th>\n",
       "      <th>Precision</th>\n",
       "      <th>#Params (B)</th>\n",
       "      <th>Flagged</th>\n",
       "      <th>MoE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>davidkim205/Rhea-72b-v0.5</td>\n",
       "      <td>81.22</td>\n",
       "      <td>79.78</td>\n",
       "      <td>91.15</td>\n",
       "      <td>77.95</td>\n",
       "      <td>74.50</td>\n",
       "      <td>87.85</td>\n",
       "      <td>76.12</td>\n",
       "      <td>float16</td>\n",
       "      <td>72.29</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contamination/contaminated_proof_7b_v1.0_safet...</td>\n",
       "      <td>81.14</td>\n",
       "      <td>78.07</td>\n",
       "      <td>90.22</td>\n",
       "      <td>78.92</td>\n",
       "      <td>82.29</td>\n",
       "      <td>88.16</td>\n",
       "      <td>69.14</td>\n",
       "      <td>float16</td>\n",
       "      <td>7.24</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contamination/contaminated_proof_7b_v1.0</td>\n",
       "      <td>81.14</td>\n",
       "      <td>78.07</td>\n",
       "      <td>90.22</td>\n",
       "      <td>78.92</td>\n",
       "      <td>82.29</td>\n",
       "      <td>88.16</td>\n",
       "      <td>69.14</td>\n",
       "      <td>float16</td>\n",
       "      <td>7.00</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>davidkim205/Rhea-72b-v0.4</td>\n",
       "      <td>81.09</td>\n",
       "      <td>78.50</td>\n",
       "      <td>90.75</td>\n",
       "      <td>78.01</td>\n",
       "      <td>73.91</td>\n",
       "      <td>86.74</td>\n",
       "      <td>78.62</td>\n",
       "      <td>float16</td>\n",
       "      <td>72.29</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MTSAIR/MultiVerse_70B</td>\n",
       "      <td>81.00</td>\n",
       "      <td>78.67</td>\n",
       "      <td>89.77</td>\n",
       "      <td>78.22</td>\n",
       "      <td>75.18</td>\n",
       "      <td>87.53</td>\n",
       "      <td>76.65</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>72.29</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  Average    ARC  \\\n",
       "0                          davidkim205/Rhea-72b-v0.5    81.22  79.78   \n",
       "1  Contamination/contaminated_proof_7b_v1.0_safet...    81.14  78.07   \n",
       "2           Contamination/contaminated_proof_7b_v1.0    81.14  78.07   \n",
       "3                          davidkim205/Rhea-72b-v0.4    81.09  78.50   \n",
       "4                              MTSAIR/MultiVerse_70B    81.00  78.67   \n",
       "\n",
       "   HellaSwag   MMLU  TruthfulQA  Winogrande  GSM8K Precision  #Params (B)  \\\n",
       "0      91.15  77.95       74.50       87.85  76.12   float16        72.29   \n",
       "1      90.22  78.92       82.29       88.16  69.14   float16         7.24   \n",
       "2      90.22  78.92       82.29       88.16  69.14   float16         7.00   \n",
       "3      90.75  78.01       73.91       86.74  78.62   float16        72.29   \n",
       "4      89.77  78.22       75.18       87.53  76.65  bfloat16        72.29   \n",
       "\n",
       "   Flagged    MoE  \n",
       "0    False  False  \n",
       "1     True  False  \n",
       "2     True  False  \n",
       "3    False  False  \n",
       "4    False  False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_llm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96feea",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70b6d6",
   "metadata": {},
   "source": [
    "La colonne Precision comprend √† la fois des data types (`float16`, `bfloat16`) et des m√©thodes de quantization(`4bit`, `8bit` et `GPTQ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f90fb9de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['float16', 'bfloat16', '4bit', '8bit', 'GPTQ', nan], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_llm_df['Precision'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8cd13a",
   "metadata": {},
   "source": [
    "Il existe plusieurs mod√®les avec plusieurs valeurs de Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb45e646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "01-ai/Yi-34B-Chat                                                2\n",
       "01-ai/Yi-6B                                                      2\n",
       "AA051610/A0106                                                   2\n",
       "AA051610/FT                                                      2\n",
       "AI-Sweden-Models/gpt-sw3-126m                                    2\n",
       "AIGym/deepseek-coder-1.3b-chat                                   2\n",
       "AIGym/deepseek-coder-6.7b-chat                                   2\n",
       "Abhaykoul/qwen1.5-vortex                                         2\n",
       "Azazelle/Bianca-7b                                               2\n",
       "BAAI/Aquila2-34B                                                 2\n",
       "BEE-spoke-data/smol_llama-101M-GQA                               2\n",
       "BarraHome/Wistral-7B-Instruct-v0.3                               2\n",
       "BryanSwk/LaserPipe-7B-SLERP                                      2\n",
       "CausalLM/72B-preview                                             2\n",
       "CausalLM/72B-preview-canary-llamafied-qwen-llamafy-unbias-qkv    2\n",
       "Changgil/k2s3_test_24001                                         2\n",
       "ConvexAI/Pelican-9b-v0.1                                         2\n",
       "ConvexAI/Seraphim-8x10.7B-bf16                                   2\n",
       "CultriX/NeuralMona_MoE-4x7B                                      2\n",
       "Dans-DiscountModels/Mistral-7b-FFT-Test3                         2\n",
       "Dans-DiscountModels/TinyMistral-v2.5-MiniPile-Guidelines-E1      2\n",
       "DatPySci/pythia-1b-sft-50k                                       2\n",
       "DrNicefellow/ChatAllInOne-Yi-34B-200K-V1                         2\n",
       "FelixChao/Faraday-7B                                             2\n",
       "FelixChao/llama2-13b-math1.1                                     2\n",
       "FelixChao/llama2-13b-math1.2                                     2\n",
       "FredrikBL/NeuralPipe-7B-slerp                                    2\n",
       "Gille/StrangeMerges_12-7B-slerp                                  2\n",
       "Gille/StrangeMerges_17-7B-dare_ties                              2\n",
       "GritLM/GritLM-7B                                                 2\n",
       "GritLM/GritLM-8x7B                                               2\n",
       "Hemanth-thunder/Tamil-Mistral-7B-Instruct-v0.1                   2\n",
       "Henk717/airochronos-33B                                          2\n",
       "HuggingFaceH4/zephyr-7b-beta                                     3\n",
       "HuggingFaceH4/zephyr-7b-gemma-v0.1                               2\n",
       "HuggingFaceTB/cosmo-1b                                           2\n",
       "Intel/neural-chat-7b-v3-1                                        4\n",
       "Isaak-Carter/J.O.S.I.E.3-Beta10-7B-slerp                         2\n",
       "Isaak-Carter/J.O.S.I.E.3-Beta11-7B-slerp                         2\n",
       "Jaume/openchat-3.5-0106-mod-gpt5                                 2\n",
       "JiheonJeong/v1                                                   2\n",
       "Kabster/Bio-Mistralv2-Squared                                    2\n",
       "KaeriJenti/Kaori-34B-v1                                          2\n",
       "KoboldAI/LLaMA2-13B-Estopia                                      2\n",
       "Kukedlc/NeuTrixOmniBe-7B-model-remix                             2\n",
       "Kukedlc/NeuTrixOmniBe-DPO                                        2\n",
       "Kukedlc/NeuralKukedlc-7B-Labonned                                2\n",
       "LDCC/LDCC-SOLAR-10.7B                                            2\n",
       "LHC88/LaseredHermes-7B-v1                                        2\n",
       "Lexic0n/Mistral_7B-Open_Hermes-NSFWV1                            2\n",
       "M4-ai/tau-0.5B                                                   2\n",
       "MTSAIR/MultiVerse_70B                                            2\n",
       "Mihaiii/Metis-0.4                                                2\n",
       "Mihaiii/Pallas-0.2                                               2\n",
       "Mihaiii/Pallas-0.3                                               2\n",
       "Mihaiii/Pallas-0.4                                               2\n",
       "Mohammed-Altaf/Medical-ChatBot                                   3\n",
       "NExtNewChattingAI/shark_tank_ai_7b_v2                            2\n",
       "NLUHOPOE/experiment2-cause                                       2\n",
       "NLUHOPOE/experiment2-cause-non                                   2\n",
       "NeverSleep/Noromaid-7B-0.4-DPO                                   2\n",
       "NeverSleep/Noromaid-7b-v0.2                                      2\n",
       "NousResearch/Hermes-2-Pro-Mistral-7B                             2\n",
       "NousResearch/Nous-Hermes-2-Mistral-7B-DPO                        2\n",
       "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO                      2\n",
       "NousResearch/Nous-Hermes-Llama2-13b                              2\n",
       "NousResearch/Redmond-Puffin-13B                                  2\n",
       "Open-Orca/OpenOrcaxOpenChat-Preview2-13B                         2\n",
       "OpenBuddy/openbuddy-mistral-7b-v17.1-32k                         2\n",
       "OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k                       2\n",
       "OpenBuddyEA/openbuddy-llama-30b-v7.1-bf16                        2\n",
       "OpenPipe/mistral-ft-optimized-1227                               2\n",
       "PetroGPT/Voldemort-10B-DPO                                       2\n",
       "PocketDoc/Dans-TotSirocco-7b                                     2\n",
       "PotatoB/MergeM-7B                                                2\n",
       "Q-bert/MetaMath-Cybertron-Starling                               2\n",
       "Qwen/Qwen1.5-72B-Chat                                            2\n",
       "Qwen/Qwen1.5-7B-Chat                                             2\n",
       "RESMPDEV/Gemma-Wukong-2b                                         2\n",
       "RESMPDEV/Gemma-Wukong1.1-2b                                      2\n",
       "SC44/Mistral-7B-private-spef                                     3\n",
       "SC44/Mistral-7B-private-spnf                                     2\n",
       "SanjiWatsuki/Kunoichi-DPO-v2-7B                                  2\n",
       "SanjiWatsuki/Loyal-Toppy-Bruins-Maid-7B-DARE                     2\n",
       "SanjiWatsuki/Sonya-7B                                            2\n",
       "Sao10K/Fimbulvetr-11B-v2                                         2\n",
       "Sao10K/Frostwind-10.7B-v1                                        2\n",
       "SeaLLMs/SeaLLM-7B-v2                                             2\n",
       "ShinojiResearch/Senku-70B-Full                                   2\n",
       "SilverCoder66/Mistral-7B-Instruct-adapt-v0.21                    2\n",
       "StudentLLM/Alpagasus-2-13b-QLoRA-merged                          2\n",
       "Telugu-LLM-Labs/Telugu-Llama2-7B-v0-Instruct                     2\n",
       "TheTravellingEngineer/llama2-7b-chat-hf-v3                       2\n",
       "TheTravellingEngineer/llama2-7b-chat-hf-v4                       2\n",
       "TinyLlama/TinyLlama-1.1B-Chat-v1.0                               2\n",
       "UCLA-AGI/test-test                                               2\n",
       "UCLA-AGI/zephyr-7b-sft-full-SPIN-iter0                           2\n",
       "VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct                 2\n",
       "Weyaxi/Dolphin2.1-OpenOrca-7B                                    2\n",
       "Weyaxi/openchat-3.5-1210-Seraph-Slerp                            2\n",
       "WizardLM/WizardLM-30B-V1.0                                       2\n",
       "WizardLM/WizardMath-70B-V1.0                                     3\n",
       "Xenon1/Zenith-7B-dpo-v1                                          2\n",
       "YouKnwMe/Mistral-7b-instruct-v0.2-private-eds2                   2\n",
       "YouKnwMe/Mistral-7b-instruct-v0.2-private-edw2                   2\n",
       "Yukang/Llama-2-13b-chat-longlora-32k-sft                         2\n",
       "Zardos/Kant-Test-0.1-Mistral-7B                                  2\n",
       "ZoidBB/Jovian-10.7B-v1.0                                         2\n",
       "abacusai/Liberated-Qwen1.5-14B                                   2\n",
       "abacusai/Liberated-Qwen1.5-72B                                   2\n",
       "abacusai/Smaug-Mixtral-v0.1                                      2\n",
       "abacusai/bigstral-12b-32k                                        2\n",
       "abhinand/malayalam-llama-7b-instruct-v0.1                        2\n",
       "abhinand/tamil-llama-7b-instruct-v0.2                            2\n",
       "abhinand/telugu-llama-7b-instruct-v0.1                           2\n",
       "abhishek/autotrain-8kfjk-b3gva                                   2\n",
       "abhishekchohan/mistral-7B-forest-merge                           2\n",
       "ajibawa-2023/Code-Mistral-7B                                     2\n",
       "alignment-handbook/zephyr-7b-sft-full                            2\n",
       "alnrg2arg/blockchainlabs_joe_bez_seminar                         2\n",
       "aloobun/open-llama-3b-v2-elmv3                                   2\n",
       "amu/spin-phi2                                                    2\n",
       "andysalerno/cloudymixtral7Bx2-nectar-0.2                         2\n",
       "argilla/notus-7b-v1                                              2\n",
       "berkeley-nest/Starling-LM-7B-alpha                               2\n",
       "bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16           2\n",
       "binbi/Ein-72B-v0.1                                               2\n",
       "chanwit/flux-base-optimized                                      2\n",
       "chargoddard/internlm2-20b-llama                                  2\n",
       "chargoddard/llama2-22b-blocktriangular                           2\n",
       "chargoddard/loyal-piano-m7-cdpo                                  2\n",
       "circulus/Llama-2-13b-orca-v1                                     2\n",
       "cloudyu/Mixtral_34Bx2_MoE_60B                                    2\n",
       "cloudyu/Mixtral_7Bx6_MoE_35B                                     2\n",
       "codellama/CodeLlama-34b-Instruct-hf                              2\n",
       "codellama/CodeLlama-34b-Python-hf                                2\n",
       "codellama/CodeLlama-34b-hf                                       2\n",
       "codellama/CodeLlama-7b-Python-hf                                 2\n",
       "cognitivecomputations/dolphin-2.2-yi-34b-200k                    2\n",
       "cognitivecomputations/dolphin-2.6-mistral-7b                     2\n",
       "cognitivecomputations/dolphin-2.8-experiment26-7b                2\n",
       "conceptofmind/LLongMA-2-13b-16k                                  2\n",
       "croissantllm/CroissantLLMBase                                    2\n",
       "cstr/Spaetzle-v8-7b                                              2\n",
       "decruz07/kellemar-DPO-7B                                         2\n",
       "decruz07/llama-2-7b-miniguanaco                                  2\n",
       "deepseek-ai/deepseek-llm-67b-chat                                2\n",
       "deepseek-ai/deepseek-llm-7b-chat                                 2\n",
       "deepseek-ai/deepseek-math-7b-instruct                            2\n",
       "dozzke/hermorca                                                  2\n",
       "dreamgen/opus-v1-34b                                             2\n",
       "dreamgen/opus-v1.2-7b                                            2\n",
       "ehartford/WizardLM-1.0-Uncensored-Llama2-13b                     2\n",
       "ehartford/dolphin-2.1-mistral-7b                                 2\n",
       "ehartford/dolphin-2.2.1-mistral-7b                               2\n",
       "ethzanalytics/pythia-31m                                         2\n",
       "euclaise/Ferret-7B                                               2\n",
       "euclaise/falcon_1b_stage2                                        2\n",
       "fblgit/UNA-SOLAR-10.7B-Instruct-v1.0                             2\n",
       "fhai50032/RP-Coder-SM3                                           2\n",
       "fhai50032/RPLakeCoder-TxC                                        2\n",
       "fhai50032/SamChat                                                2\n",
       "fhai50032/SamCoder-TxC                                           2\n",
       "fhai50032/xLakeChat                                              2\n",
       "gagan3012/MetaModel_moe                                          2\n",
       "gaodrew/gaodrew-gorgonzola-13b                                   2\n",
       "garage-bAInd/Camel-Platypus2-70B                                 2\n",
       "gemmathon/gemma-pro-2.8b-ko-v0                                   2\n",
       "gemmathon/gemma-pro-3.1b-ko-v0.1                                 2\n",
       "genaicore3434/Mistral-7b-instruct-v0.2-summ-sft-lp-e1            2\n",
       "genaicore3434/MistralLite-summ-sft-e1                            2\n",
       "gmonsoon/Qwenchana-0.5B-restart                                  2\n",
       "google/gemma-2b                                                  2\n",
       "google/gemma-7b                                                  2\n",
       "google/recurrentgemma-2b                                         2\n",
       "gpt2                                                             2\n",
       "gradientai/v-alpha-tross                                         2\n",
       "hamxea/Llama-2-7b-chat-hf-activity-fine-tuned-v4                 2\n",
       "harborwater/open-llama-3b-v2-wizard-evol-instuct-v2-196k         2\n",
       "hfl/chinese-mixtral                                              2\n",
       "huseyinatahaninan/phi-2-instruction                              2\n",
       "ichigoberry/MonarchPipe-7B-slerp                                 2\n",
       "ichigoberry/pandafish-7b                                         2\n",
       "itsliupeng/llama_9b_long                                         2\n",
       "jan-hq/stealth-v1.3                                              2\n",
       "jefferylovely/AthenaImaniMaven                                   2\n",
       "jilp00/OpenHermes-Symbolic-Mistral-7B                            2\n",
       "jondurbin/airoboros-33b-gpt4-1.3                                 2\n",
       "jondurbin/airoboros-33b-gpt4-2.0                                 2\n",
       "jondurbin/airoboros-33b-gpt4-m2.0                                3\n",
       "jondurbin/airoboros-65b-gpt4-1.4                                 2\n",
       "jondurbin/airoboros-65b-gpt4-2.0                                 2\n",
       "jondurbin/airoboros-65b-gpt4-m2.0                                2\n",
       "jondurbin/airoboros-l2-13b-2.1                                   2\n",
       "jondurbin/airoboros-l2-70b-gpt4-2.0                              2\n",
       "jondurbin/bagel-dpo-34b-v0.2                                     2\n",
       "kfkas/Llama-2-ko-7b-Chat                                         2\n",
       "kno10/ende-chat-0.0.4                                            2\n",
       "lemon-mint/gemma-ko-7b-instruct-v0.62                            2\n",
       "lex-hue/Delexa-7b                                                2\n",
       "lgaalves/gpt2-dolly                                              2\n",
       "liminerity/Blur-7b-v1.22                                         2\n",
       "llm-agents/tora-13b-v1.0                                         2\n",
       "llm-agents/tora-70b-v1.0                                         2\n",
       "llm-agents/tora-code-13b-v1.0                                    2\n",
       "llm-agents/tora-code-34b-v1.0                                    2\n",
       "lmsys/vicuna-7b-v1.5                                             2\n",
       "lmsys/vicuna-7b-v1.5-16k                                         2\n",
       "lodrick-the-lafted/Grafted-Hermetic-Platypus-C-2x7B              2\n",
       "luffycodes/mcq-vicuna-13b-v1.5                                   2\n",
       "macadeliccc/Monarch-7B-SFT                                       2\n",
       "macadeliccc/OmniCorso-7B                                         2\n",
       "maldv/dragonwar-7b-s1                                            2\n",
       "martyn/mixtral-megamerge-dare-8x7b-v2                            2\n",
       "mathurinache/Odysseas-11B                                        2\n",
       "maywell/Synatra-RP-Orca-2-7b-v0.1                                2\n",
       "microsoft/Orca-2-13b                                             2\n",
       "migtissera/Tess-XS-v1-3-yarn-128K                                2\n",
       "mindy-labs/mindy-7b                                              2\n",
       "mistralai/Mixtral-8x7B-Instruct-v0.1                             2\n",
       "mistralai/Mixtral-8x7B-v0.1                                      2\n",
       "mlabonne/NeuralPipe-7B-slerp                                     2\n",
       "moreh/MoMo-72B-LoRA-V1.4                                         2\n",
       "mosaicml/mpt-7b-8k-instruct                                      2\n",
       "mrm8488/mistral-7b-ft-h4-no_robots_instructions                  2\n",
       "nlpguy/Hermes-low-tune-2                                         2\n",
       "nnheui/pythia-410m-sft-full                                      2\n",
       "one-man-army/una-neural-chat-v3-3-P2-OMA                         2\n",
       "openaccess-ai-collective/DPOpenHermes-7B                         2\n",
       "openagi-project/OpenAGI-7B-v0.1                                  2\n",
       "openchat/openchat_3.5                                            3\n",
       "openchat/openchat_v3.1                                           2\n",
       "openchat/openchat_v3.2                                           2\n",
       "pabloce/Dolphin-2.8-slerp                                        2\n",
       "pandego/my-first-blend                                           2\n",
       "pinkyponky/Mistral-7b-instruct-v0.2-summ-sft-e1                  2\n",
       "pinkyponky/Mistral-7b-instruct-v0.2-summ-sft-e2                  2\n",
       "pinkyponky/Mistral-7b-instruct-v0.2-summ-sft-e3                  2\n",
       "pmking27/PrathameshLLM-7B                                        2\n",
       "prince-canuma/Damysus-2.7B-Chat                                  2\n",
       "qblocks/falcon_7b_norobots                                       2\n",
       "r2rss/Malachite-7b-v0                                            2\n",
       "rombodawg/Everyone-Coder-4x7b-Base                               2\n",
       "royallab/PsyOrca2-13b-DARE                                       2\n",
       "rwitz/dec10                                                      2\n",
       "rwitz/go-bruins                                                  2\n",
       "rwitz/go-bruins-v2                                               2\n",
       "sail/Sailor-0.5B                                                 2\n",
       "sail/Sailor-4B                                                   2\n",
       "sail/Sailor-7B                                                   2\n",
       "saltlux/luxia-21.4b-alignment-v1.1                               2\n",
       "silvercoder67/Mistral-7b-instruct-v0.2-summ-sft-dpo-e1           2\n",
       "silvercoder67/Mistral-7b-instruct-v0.2-summ-sft-dpo-e2           2\n",
       "snorkelai/Snorkel-Mistral-PairRM-DPO                             2\n",
       "spmurrayzzz/Mistral-Syndicate-7B                                 2\n",
       "sr5434/CodegebraGPT-10b                                          2\n",
       "team-lucid/mptk-1b                                               2\n",
       "teknium/OpenHermes-2.5-Mistral-7B                                2\n",
       "tianlinliu0121/zephyr-7b-dpo-full-beta-0.2                       2\n",
       "tiiuae/falcon-180B                                               2\n",
       "tiiuae/falcon-7b-instruct                                        2\n",
       "togethercomputer/Llama-2-7B-32K-Instruct                         2\n",
       "tushar310/Hippy-AAI-7B                                           2\n",
       "tushar310/MisGemma-7B                                            2\n",
       "tyson0420/stack_llama_fil_ai                                     2\n",
       "upaya07/Birbal-7B-V1                                             2\n",
       "uukuguy/GDC-Tiny-L1-1.8B                                         2\n",
       "uukuguy/speechless-code-mistral-7b-v1.0                          2\n",
       "uukuguy/speechless-code-mistral-7b-v2.0                          2\n",
       "uukuguy/speechless-codellama-orca-13b                            2\n",
       "uukuguy/speechless-codellama-orca-airoboros-13b-0.10e            2\n",
       "uukuguy/speechless-codellama-orca-platypus-13b-0.10e             2\n",
       "uukuguy/speechless-codellama-platypus-13b                        2\n",
       "uukuguy/speechless-hermes-coig-lite-13b                          2\n",
       "uukuguy/speechless-mistral-hermes-code-7b                        2\n",
       "vihangd/neuralfalcon-1b-v1                                       2\n",
       "walebadr/Mistral-7B-v0.1-DPO                                     2\n",
       "wenge-research/yayi-7b-llama2                                    2\n",
       "wtang06/mpt-125m-c4                                              2\n",
       "yam-peleg/Experiment26-7B                                        2\n",
       "yam-peleg/Experiment30-7B                                        2\n",
       "yam-peleg/Experiment31-7B                                        2\n",
       "yanolja/Bookworm-10.7B-v0.4-DPO                                  2\n",
       "yhyhy3/med-orca-instruct-33b                                     2\n",
       "zelus82/Obelix-Phi2                                              2\n",
       "ziniuli/Mistral-7B-ReMax-v0.1                                    2\n",
       "Name: Precision, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_precisions_per_model = open_llm_df.groupby('model')['Precision'].nunique()\n",
    "models_with_multiple_precision = unique_precisions_per_model[unique_precisions_per_model > 1]\n",
    "models_with_multiple_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb3a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_perf_filtered_df = pd.read_csv('./data/llm_perf_filtered.csv', index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec85e6",
   "metadata": {},
   "source": [
    "Si l'on croise avec les mod√®les de LLM Perf, il n'y a que `CodeLlama-34b-hf` qui est concern√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b1772ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'codellama/CodeLlama-34b-hf'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(llm_perf_filtered_df['model']).intersection(set(models_with_multiple_precision.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce59190",
   "metadata": {},
   "source": [
    "Seul le mod√®le `Yi-34B-200K` manque dans le dataset d'Open LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e04fcbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'01-ai/Yi-34B-200K'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(llm_perf_filtered_df['model'].unique()) - set(open_llm_df['model'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2b49a",
   "metadata": {},
   "source": [
    "### Merge avec LLM Perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e319fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm_perf_filtered_df = llm_perf_filtered_df.merge(open_llm_df, how='inner', on='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03bb02df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>throughput</th>\n",
       "      <th>response_length</th>\n",
       "      <th>latency</th>\n",
       "      <th>energy</th>\n",
       "      <th>gpu</th>\n",
       "      <th>task</th>\n",
       "      <th>parameters_count</th>\n",
       "      <th>energy_per_token</th>\n",
       "      <th>dtype</th>\n",
       "      <th>optimization</th>\n",
       "      <th>quantization</th>\n",
       "      <th>cuda-fp16</th>\n",
       "      <th>Average</th>\n",
       "      <th>ARC</th>\n",
       "      <th>HellaSwag</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>TruthfulQA</th>\n",
       "      <th>Winogrande</th>\n",
       "      <th>GSM8K</th>\n",
       "      <th>Precision</th>\n",
       "      <th>#Params (B)</th>\n",
       "      <th>Flagged</th>\n",
       "      <th>MoE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EleutherAI/gpt-neox-20b</td>\n",
       "      <td>27.4</td>\n",
       "      <td>256</td>\n",
       "      <td>9.34</td>\n",
       "      <td>3013.632</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00327</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gptq-4bit+exllama-v1</td>\n",
       "      <td>False</td>\n",
       "      <td>41.69</td>\n",
       "      <td>45.73</td>\n",
       "      <td>73.45</td>\n",
       "      <td>25.00</td>\n",
       "      <td>31.61</td>\n",
       "      <td>68.90</td>\n",
       "      <td>5.46</td>\n",
       "      <td>float16</td>\n",
       "      <td>20.74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EleutherAI/gpt-neox-20b</td>\n",
       "      <td>25.6</td>\n",
       "      <td>256</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2995.200</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00325</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awq-4bit+gemv</td>\n",
       "      <td>False</td>\n",
       "      <td>41.69</td>\n",
       "      <td>45.73</td>\n",
       "      <td>73.45</td>\n",
       "      <td>25.00</td>\n",
       "      <td>31.61</td>\n",
       "      <td>68.90</td>\n",
       "      <td>5.46</td>\n",
       "      <td>float16</td>\n",
       "      <td>20.74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EleutherAI/gpt-neox-20b</td>\n",
       "      <td>27.5</td>\n",
       "      <td>256</td>\n",
       "      <td>9.32</td>\n",
       "      <td>2598.912</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00282</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gptq-4bit+exllama-v2</td>\n",
       "      <td>False</td>\n",
       "      <td>41.69</td>\n",
       "      <td>45.73</td>\n",
       "      <td>73.45</td>\n",
       "      <td>25.00</td>\n",
       "      <td>31.61</td>\n",
       "      <td>68.90</td>\n",
       "      <td>5.46</td>\n",
       "      <td>float16</td>\n",
       "      <td>20.74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EleutherAI/gpt-neox-20b</td>\n",
       "      <td>23.5</td>\n",
       "      <td>256</td>\n",
       "      <td>10.90</td>\n",
       "      <td>3216.384</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00349</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awq-4bit+gemm</td>\n",
       "      <td>False</td>\n",
       "      <td>41.69</td>\n",
       "      <td>45.73</td>\n",
       "      <td>73.45</td>\n",
       "      <td>25.00</td>\n",
       "      <td>31.61</td>\n",
       "      <td>68.90</td>\n",
       "      <td>5.46</td>\n",
       "      <td>float16</td>\n",
       "      <td>20.74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EleutherAI/pythia-12b</td>\n",
       "      <td>36.6</td>\n",
       "      <td>256</td>\n",
       "      <td>6.99</td>\n",
       "      <td>2101.248</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.00228</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gptq-4bit+exllama-v1</td>\n",
       "      <td>False</td>\n",
       "      <td>38.82</td>\n",
       "      <td>39.59</td>\n",
       "      <td>68.82</td>\n",
       "      <td>26.76</td>\n",
       "      <td>31.85</td>\n",
       "      <td>64.17</td>\n",
       "      <td>1.74</td>\n",
       "      <td>float16</td>\n",
       "      <td>12.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  throughput  response_length  latency    energy  \\\n",
       "0  EleutherAI/gpt-neox-20b        27.4              256     9.34  3013.632   \n",
       "1  EleutherAI/gpt-neox-20b        25.6              256    10.00  2995.200   \n",
       "2  EleutherAI/gpt-neox-20b        27.5              256     9.32  2598.912   \n",
       "3  EleutherAI/gpt-neox-20b        23.5              256    10.90  3216.384   \n",
       "4    EleutherAI/pythia-12b        36.6              256     6.99  2101.248   \n",
       "\n",
       "                     gpu  task  parameters_count  energy_per_token    dtype  \\\n",
       "0  NVIDIA A100-SXM4-80GB  chat              20.0           0.00327  float16   \n",
       "1  NVIDIA A100-SXM4-80GB  chat              20.0           0.00325  float16   \n",
       "2  NVIDIA A100-SXM4-80GB  chat              20.0           0.00282  float16   \n",
       "3  NVIDIA A100-SXM4-80GB  chat              20.0           0.00349  float16   \n",
       "4  NVIDIA A100-SXM4-80GB  chat              12.0           0.00228  float16   \n",
       "\n",
       "   optimization          quantization  cuda-fp16  Average    ARC  HellaSwag  \\\n",
       "0           NaN  gptq-4bit+exllama-v1      False    41.69  45.73      73.45   \n",
       "1           NaN         awq-4bit+gemv      False    41.69  45.73      73.45   \n",
       "2           NaN  gptq-4bit+exllama-v2      False    41.69  45.73      73.45   \n",
       "3           NaN         awq-4bit+gemm      False    41.69  45.73      73.45   \n",
       "4           NaN  gptq-4bit+exllama-v1      False    38.82  39.59      68.82   \n",
       "\n",
       "    MMLU  TruthfulQA  Winogrande  GSM8K Precision  #Params (B)  Flagged    MoE  \n",
       "0  25.00       31.61       68.90   5.46   float16        20.74    False  False  \n",
       "1  25.00       31.61       68.90   5.46   float16        20.74    False  False  \n",
       "2  25.00       31.61       68.90   5.46   float16        20.74    False  False  \n",
       "3  25.00       31.61       68.90   5.46   float16        20.74    False  False  \n",
       "4  26.76       31.85       64.17   1.74   float16        12.00    False  False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_llm_perf_filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0acca",
   "metadata": {},
   "source": [
    "#### Analyse de param√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50521b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_params = open_llm_perf_filtered_df[np.abs(open_llm_perf_filtered_df['parameters_count'] - open_llm_perf_filtered_df['#Params (B)']) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a5cd6af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>parameters_count</th>\n",
       "      <th>#Params (B)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NYTK/PULI-GPTrio</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NYTK/PULI-GPTrio</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NYTK/PULI-GPTrio</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NYTK/PULI-GPTrio</td>\n",
       "      <td>7.67</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Writer/palmyra-large</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Writer/palmyra-large</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Writer/palmyra-large</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>golaxy/gowizardlm</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>golaxy/gowizardlm</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>golaxy/gowizardlm</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>golaxy/gowizardlm</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>golaxy/gowizardlm</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>KnutJaegersberg/Qwen-1_8B-Llamafied</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>KnutJaegersberg/Qwen-1_8B-Llamafied</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>KnutJaegersberg/Qwen-1_8B-Llamafied</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model  parameters_count  #Params (B)\n",
       "21                      NYTK/PULI-GPTrio              7.67         0.00\n",
       "22                      NYTK/PULI-GPTrio              7.67         0.00\n",
       "23                      NYTK/PULI-GPTrio              7.67         0.00\n",
       "24                      NYTK/PULI-GPTrio              7.67         0.00\n",
       "80                  Writer/palmyra-large             20.00         0.00\n",
       "81                  Writer/palmyra-large             20.00         0.00\n",
       "82                  Writer/palmyra-large             20.00         0.00\n",
       "188                    golaxy/gowizardlm              7.00         0.00\n",
       "189                    golaxy/gowizardlm              7.00         0.00\n",
       "190                    golaxy/gowizardlm              7.00         0.00\n",
       "191                    golaxy/gowizardlm              7.00         0.00\n",
       "192                    golaxy/gowizardlm              7.00         0.00\n",
       "257  KnutJaegersberg/Qwen-1_8B-Llamafied              8.00         1.84\n",
       "258  KnutJaegersberg/Qwen-1_8B-Llamafied              8.00         1.84\n",
       "259  KnutJaegersberg/Qwen-1_8B-Llamafied              8.00         1.84"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_params[['model', 'parameters_count', '#Params (B)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92337528",
   "metadata": {},
   "source": [
    "Pour les trois premiers mod√®les, ce doit √™tre un probl√®me de qualit√© des donn√©es. En revanche, pour Qwen-1_8B-Llamafied, la model card sur huggingface pr√©cise bien 1.84 md de param√®tres. Je propose de l'exclure du p√©rim√®tre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3dc3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm_perf_filtered_df = open_llm_perf_filtered_df[open_llm_perf_filtered_df['model'] != 'KnutJaegersberg/Qwen-1_8B-Llamafied']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4808e",
   "metadata": {},
   "source": [
    "Il n'y a aucun mod√®le \"flagg√©\", i.e. soumis √† discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f97d04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(open_llm_perf_filtered_df['Flagged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d7428",
   "metadata": {},
   "source": [
    "Il y a 4 mod√®les identifi√©s en tant que MoE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd407cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['uukuguy/Orca-2-7b-f16', '01-ai/Yi-34B', 'rishiraj/CatPPT-base',\n",
       "       'upstage/SOLAR-10.7B-v1.0'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_llm_perf_filtered_df.loc[open_llm_perf_filtered_df['MoE'], 'model'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b67042",
   "metadata": {},
   "source": [
    "Au vu des correspondances ci-dessous, la colonne Precision n'a pas l'air tr√®s pr√©cise ^^'. Je propose de ne pas en tenir compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a002a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype    quantization          Precision\n",
       "float16  awq-4bit+gemm         float16      51\n",
       "         awq-4bit+gemv         float16      50\n",
       "         gptq-4bit+exllama-v1  float16      47\n",
       "         gptq-4bit+exllama-v2  float16      42\n",
       "         gptq-4bit             float16      18\n",
       "         awq-4bit+gemm         bfloat16     13\n",
       "         awq-4bit+gemv         bfloat16     12\n",
       "         gptq-4bit+exllama-v1  bfloat16     11\n",
       "         gptq-4bit+exllama-v2  bfloat16     10\n",
       "         gptq-4bit             bfloat16      4\n",
       "         awq-4bit+gemm         4bit          1\n",
       "         awq-4bit+gemv         4bit          1\n",
       "         gptq-4bit             4bit          1\n",
       "         gptq-4bit+exllama-v1  4bit          1\n",
       "         gptq-4bit+exllama-v2  4bit          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_llm_perf_filtered_df.value_counts(['dtype', 'quantization', 'Precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb9a0d",
   "metadata": {},
   "source": [
    "Le mod√®le `CodeLlama-34b-hf` est le seul avec plusieurs valeurs de `Precision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0fb63ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>throughput</th>\n",
       "      <th>response_length</th>\n",
       "      <th>latency</th>\n",
       "      <th>energy</th>\n",
       "      <th>gpu</th>\n",
       "      <th>task</th>\n",
       "      <th>parameters_count</th>\n",
       "      <th>energy_per_token</th>\n",
       "      <th>dtype</th>\n",
       "      <th>optimization</th>\n",
       "      <th>quantization</th>\n",
       "      <th>cuda-fp16</th>\n",
       "      <th>Average</th>\n",
       "      <th>ARC</th>\n",
       "      <th>HellaSwag</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>TruthfulQA</th>\n",
       "      <th>Winogrande</th>\n",
       "      <th>GSM8K</th>\n",
       "      <th>Precision</th>\n",
       "      <th>#Params (B)</th>\n",
       "      <th>Flagged</th>\n",
       "      <th>MoE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>codellama/CodeLlama-34b-hf</td>\n",
       "      <td>24.4</td>\n",
       "      <td>256</td>\n",
       "      <td>10.5</td>\n",
       "      <td>3741.696</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awq-4bit+gemv</td>\n",
       "      <td>False</td>\n",
       "      <td>55.33</td>\n",
       "      <td>54.10</td>\n",
       "      <td>75.82</td>\n",
       "      <td>55.02</td>\n",
       "      <td>39.11</td>\n",
       "      <td>73.56</td>\n",
       "      <td>34.34</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>33.74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>codellama/CodeLlama-34b-hf</td>\n",
       "      <td>24.4</td>\n",
       "      <td>256</td>\n",
       "      <td>10.5</td>\n",
       "      <td>3741.696</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awq-4bit+gemv</td>\n",
       "      <td>False</td>\n",
       "      <td>55.28</td>\n",
       "      <td>54.18</td>\n",
       "      <td>75.82</td>\n",
       "      <td>54.92</td>\n",
       "      <td>39.11</td>\n",
       "      <td>73.32</td>\n",
       "      <td>34.34</td>\n",
       "      <td>float16</td>\n",
       "      <td>33.74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>codellama/CodeLlama-34b-hf</td>\n",
       "      <td>22.7</td>\n",
       "      <td>256</td>\n",
       "      <td>11.3</td>\n",
       "      <td>3861.504</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awq-4bit+gemm</td>\n",
       "      <td>False</td>\n",
       "      <td>55.33</td>\n",
       "      <td>54.10</td>\n",
       "      <td>75.82</td>\n",
       "      <td>55.02</td>\n",
       "      <td>39.11</td>\n",
       "      <td>73.56</td>\n",
       "      <td>34.34</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>33.74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>codellama/CodeLlama-34b-hf</td>\n",
       "      <td>22.7</td>\n",
       "      <td>256</td>\n",
       "      <td>11.3</td>\n",
       "      <td>3861.504</td>\n",
       "      <td>NVIDIA A100-SXM4-80GB</td>\n",
       "      <td>chat</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>float16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>awq-4bit+gemm</td>\n",
       "      <td>False</td>\n",
       "      <td>55.28</td>\n",
       "      <td>54.18</td>\n",
       "      <td>75.82</td>\n",
       "      <td>54.92</td>\n",
       "      <td>39.11</td>\n",
       "      <td>73.32</td>\n",
       "      <td>34.34</td>\n",
       "      <td>float16</td>\n",
       "      <td>33.74</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  throughput  response_length  latency  \\\n",
       "262  codellama/CodeLlama-34b-hf        24.4              256     10.5   \n",
       "263  codellama/CodeLlama-34b-hf        24.4              256     10.5   \n",
       "264  codellama/CodeLlama-34b-hf        22.7              256     11.3   \n",
       "265  codellama/CodeLlama-34b-hf        22.7              256     11.3   \n",
       "\n",
       "       energy                    gpu  task  parameters_count  \\\n",
       "262  3741.696  NVIDIA A100-SXM4-80GB  chat              34.0   \n",
       "263  3741.696  NVIDIA A100-SXM4-80GB  chat              34.0   \n",
       "264  3861.504  NVIDIA A100-SXM4-80GB  chat              34.0   \n",
       "265  3861.504  NVIDIA A100-SXM4-80GB  chat              34.0   \n",
       "\n",
       "     energy_per_token    dtype  optimization   quantization  cuda-fp16  \\\n",
       "262           0.00406  float16           NaN  awq-4bit+gemv      False   \n",
       "263           0.00406  float16           NaN  awq-4bit+gemv      False   \n",
       "264           0.00419  float16           NaN  awq-4bit+gemm      False   \n",
       "265           0.00419  float16           NaN  awq-4bit+gemm      False   \n",
       "\n",
       "     Average    ARC  HellaSwag   MMLU  TruthfulQA  Winogrande  GSM8K  \\\n",
       "262    55.33  54.10      75.82  55.02       39.11       73.56  34.34   \n",
       "263    55.28  54.18      75.82  54.92       39.11       73.32  34.34   \n",
       "264    55.33  54.10      75.82  55.02       39.11       73.56  34.34   \n",
       "265    55.28  54.18      75.82  54.92       39.11       73.32  34.34   \n",
       "\n",
       "    Precision  #Params (B)  Flagged    MoE  \n",
       "262  bfloat16        33.74    False  False  \n",
       "263   float16        33.74    False  False  \n",
       "264  bfloat16        33.74    False  False  \n",
       "265   float16        33.74    False  False  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_llm_perf_filtered_df[open_llm_perf_filtered_df['model'] == 'codellama/CodeLlama-34b-hf']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770c44e",
   "metadata": {},
   "source": [
    "Je propose de garder `float16` en coh√©rence avec notre filtrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65bd53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_drop = open_llm_perf_filtered_df[\n",
    "    (open_llm_perf_filtered_df['model'] == 'codellama/CodeLlama-34b-hf') & \n",
    "    (open_llm_perf_filtered_df['Precision'] == 'bfloat16')\n",
    "]\n",
    "\n",
    "open_llm_perf_filtered_df = open_llm_perf_filtered_df.drop(rows_to_drop.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86752948",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm_perf_filtered_df.to_csv('./data/open_llm_perf_filtered.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
